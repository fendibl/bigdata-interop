steps:
  # 1. Create a Docker image containing bigdata-interop repo
  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-build'
    args: ['build', '--tag=gcr.io/$PROJECT_ID/dataproc-bigdata-interop-presubmit', '-f', 'cloudbuild/Dockerfile', '.']

  # 2. Run Hadoop 2 unit tests concurrently
  - name: 'gcr.io/$PROJECT_ID/dataproc-bigdata-interop-presubmit'
    id: 'unit-tests-hadoop2'
    waitFor: ['docker-build']
    entrypoint: 'bash'
    args: ['/bigdata-interop/cloudbuild/presubmit.sh', 'hadoop2']
    env:
      - 'CODECOV_TOKEN=$_CODECOV_TOKEN'
      - 'VCS_BRANCH_NAME=$BRANCH_NAME'
      - 'VCS_COMMIT_ID=$COMMIT_SHA'
      - 'VCS_TAG=$TAG_NAME'
      - 'CI_BUILD_ID=$BUILD_ID'
  #      - 'GCS_TEST_PROJECT_ID=$PROJECT_ID'
  #      - 'GCS_TEST_SERVICE_ACCOUNT=$_GCS_TEST_SERVICE_ACCOUNT'
  #      - 'GCS_TEST_PRIVATE_KEYFILE=$_GCS_TEST_PRIVATE_KEYFILE'
  #      - 'HDFS_ROOT=file:///tmp'

  # 3. Run Hadoop 3 unit tests concurrently
  - name: 'gcr.io/$PROJECT_ID/dataproc-bigdata-interop-presubmit'
    id: 'unit-tests-hadoop3'
    waitFor: ['docker-build']
    entrypoint: 'bash'
    args: ['/bigdata-interop/cloudbuild/presubmit.sh', 'hadoop3']
    env:
#      - 'GCS_TEST_PROJECT_ID=$PROJECT_ID'
#      - 'GCS_TEST_SERVICE_ACCOUNT=$_GCS_TEST_SERVICE_ACCOUNT'
#      - 'GCS_TEST_PRIVATE_KEYFILE=$_GCS_TEST_PRIVATE_KEYFILE'
#      - 'HDFS_ROOT=file:///tmp'

options:
  machineType: 'N1_HIGHCPU_8'

